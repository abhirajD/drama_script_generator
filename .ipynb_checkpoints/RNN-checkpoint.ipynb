{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVIE SCRIPT GENERATOR\n",
    "\n",
    "In this Notebook, I give a seq2seq LSTM network word embeddings of starwars movie scripts. I have explored multiple scripts from movies dbs, TV series (even tried generating dataset for my favourite series: [Elementary](https://en.wikipedia.org/wiki/Elementary_(TV_series)) etc. but we need data over which we can abstract well.\n",
    "\n",
    "Abitity to abstract over data is usually determined by three factors:\n",
    "1. The Network itself\n",
    "2. The amount and quality of data\n",
    "3. Regularizations applied\n",
    "\n",
    "For the 2nd point we mentioned, [Star Wars Dataset](https://www.kaggle.com/xvivancos/star-wars-movie-scripts) is the best I found for now. \n",
    "\n",
    "I will be experimenting with 1 and 3 to make the script generated by the network make sense. Also, possibly have different generations of similar network. It will be interesting to observe the concepts the network abstracts and abstracts over. Given the same network, there will be stochasticity in same networks due to curriculum of teaching.\n",
    "\n",
    "### A tiny note on Curriculum managment\n",
    "Curriculum managment is one of my favourite topics to talk about. Curriculum is usually ignored in Data Science related industrial application, but the scientific community knows very well it makes all the difference. Suppose you are designing a network to solve 'Calculus', an obvious way to go about doing it is to show the network basic calculus problems before showing it difficult problems. If the network has no knowledge of basic calculus it wont have any concepts to work with difficult problems and will fail terribly.\n",
    "\n",
    "The point of above paragraph is that its extremely difficult to conduct this kind of curriculum managment with texts. Networks generating scripts which make no sense to humans is an indication of the same. Only solution to this problem is ton of data(way more than normally required).\n",
    "\n",
    "Also, its a note to myself to improve over this model as and when I complete my research on Curriculum managment or Knowledge Distillation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and Prep Data\n",
    "\n",
    "Star Wars data looks like:\n",
    "`\"<Dialog Number>\" \"<Speaker>\" \"<LINE>\"`\n",
    "\n",
    "Expected format: \n",
    "`\"<Speaker>\" \"<LINE>\"`\n",
    "\n",
    "- `clean_srt` function does that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset/starwars'\n",
    "files = ['SW_EpisodeIV.txt', 'SW_EpisodeV.txt', 'SW_EpisodeVI.txt']\n",
    "\n",
    "def clean_srt(data_file):\n",
    "  with open(data_file) as f:\n",
    "    data = f.read()\n",
    "\n",
    "  data = data.split('\\n')\n",
    "  data = data[1:]\n",
    "  \n",
    "  #Remove Dialog Number\n",
    "  import re\n",
    "  dialog_no_pattern = re.compile(r'\"\\d+\" ')  \n",
    "  data = [dialog_no_pattern.sub('', line) for line in data]\n",
    "  \n",
    "  return '\\n'.join(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Load data\n",
    "\n",
    "`data` is a dict of `<movie>`:`<script>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies included in Data:dict_keys(['SW_EpisodeIV.txt', 'SW_EpisodeV.txt', 'SW_EpisodeVI.txt'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data = {file:clean_srt( os.path.join(data_dir, file) ) for file in files}\n",
    "print('Movies included in Data:{}'.format(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "We work with episode IV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 3368\n",
      "Number of lines: 1011\n",
      "Average number of words in each line: 12.277942631058359\n",
      "\n",
      "The sentences 0 to 5:\n",
      "\"THREEPIO\" \"Did you hear that?  They've shut down the main reactor.  We'll be destroyed for sure.  This is madness!\"\n",
      "\"THREEPIO\" \"We're doomed!\"\n",
      "\"THREEPIO\" \"There'll be no escape for the Princess this time.\"\n",
      "\"THREEPIO\" \"What's that?\"\n",
      "\"THREEPIO\" \"I should have known better than to trust the logic of a half-sized thermocapsulary dehousing assister...\"\n"
     ]
    }
   ],
   "source": [
    "text = data['SW_EpisodeIV.txt']\n",
    "view_sentence_range = (0, 5)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "sentences = [sentence for sentence in text.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "2 parts to this:\n",
    "- Lookup Table\n",
    "- Tokenizing Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lookup Table\n",
    "\n",
    "To create a word embedding, we first need to transform the words to ids. In this function, we create two dictionaries:\n",
    "\n",
    "- Dictionary to go from the words to an id, we'll call vocab_to_int\n",
    "- Dictionary to go from the id to word, we'll call int_to_vocab\n",
    "\n",
    "Then we return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Creates lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Punctuations like periods and exclamation marks make it hard to distinguish between the word like \"bye\" and \"bye!\".\n",
    "\n",
    "- `token_lookup` will return dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    d['.'] =  '||PERIOD||'\n",
    "    d[','] =  '||COMMA||'\n",
    "    d['\"'] =  '||QUOTATION_MARK||'\n",
    "    d[';'] =  '||SEMICOLON||'\n",
    "    d['!'] =  '||EXCLAMATION_MARK||'\n",
    "    d['?'] =  '||QUESTION_MARK||'\n",
    "    d['('] =  '||LEFT_PAREN||'\n",
    "    d[')'] =  '||RIGHT_PAREN||'\n",
    "    d['--'] =  '||DASH||'\n",
    "    d['?'] =  '||QUESTION_MARK>'\n",
    "    d['\\n'] =  '||NEW_LINE||'\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING THE NETWORK\n",
    "\n",
    "Now comes the interesting part.. yumm!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiraj/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
