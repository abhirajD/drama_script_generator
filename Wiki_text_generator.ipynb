{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiraj/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#dependencies\n",
    "\n",
    "import numpy as np #vectorization\n",
    "import random #Distrubution:generate text\n",
    "import tensorflow as tf #ML\n",
    "import datetime #clockTrainTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text len (in chars) 1288556\n",
      "head of text\n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 20\n"
     ]
    }
   ],
   "source": [
    "text = open('dataset/wikitext-103-raw/wiki.test.raw').read()\n",
    "print('text len (in chars)', len(text))\n",
    "\n",
    "print('head of text')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars 259\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '¥', '©', '°', '½', 'Á', 'Æ', 'É', '×', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', 'ć', 'č', 'ě', 'ī', 'ł', 'Ō', 'ō', 'Š', 'ū', 'ž', 'ǐ', 'ǔ', 'ǜ', 'ə', 'ɛ', 'ɪ', 'ʊ', 'ˈ', 'ː', '̍', '͘', 'Π', 'Ω', 'έ', 'α', 'β', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ω', 'ό', 'П', 'в', 'д', 'и', 'к', 'н', 'א', 'ב', 'י', 'ל', 'ר', 'ש', 'ת', 'ا', 'ت', 'د', 'س', 'ك', 'ل', 'و', 'ڠ', 'ग', 'न', 'र', 'ल', 'ष', 'ु', 'े', 'ो', '्', 'ả', 'ẩ', '‑', '–', '—', '’', '“', '”', '†', '‡', '…', '⁄', '₩', '₱', '→', '−', '♯', 'の', 'ア', 'イ', 'ク', 'グ', 'ジ', 'ダ', 'ッ', 'ド', 'ナ', 'ブ', 'ラ', 'ル', '中', '为', '伊', '傳', '八', '利', '前', '勢', '史', '型', '士', '大', '学', '宝', '开', '律', '成', '戦', '春', '智', '望', '杜', '東', '民', '王', '甫', '田', '甲', '秘', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '都', '鉄', '集', '魯']\n"
     ]
    }
   ],
   "source": [
    "#print out our chars and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of chars', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = dict((c,i) for i,c in enumerate(chars))\n",
    "id2char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate probability of each next character\n",
    "def sample(prediction):\n",
    "  r = random.uniform(0,1)\n",
    "  #store prediction character\n",
    "  s = 0\n",
    "  char_id = len(prediction) - 1\n",
    "  \n",
    "  for i in range(len(prediction)): \n",
    "    s += prediction[i]\n",
    "    if s >= r:\n",
    "      char_id = i\n",
    "      break\n",
    "      \n",
    "  char_one_hot = np.zeros(shape[char_size])\n",
    "  char_one_hot[char_id] = 1.0\n",
    "  return char_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRY RUN: SAMPLE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = 'abhiraj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24453480046817844"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = random.uniform(0,1)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "char_id = len(prediction) - 1\n",
    "char_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-da57b7be5fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mchar_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction)): \n",
    "  s += prediction[i]\n",
    "  print(s)\n",
    "  if s >= r:\n",
    "    char_id = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----- DONE: DRY RUN: SAMPLE -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize data\n",
    "\n",
    "len_per_section = 50\n",
    "skip = 2 # Ideally should be 50 but due to lack of data use 2\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "  sections.append(text[i: i + len_per_section])\n",
    "  next_chars.append(text[i + len_per_section])\n",
    "  \n",
    "#Vectorize\n",
    "X = np.zeros((len(sections), len_per_section, char_size), dtype = np.float32)\n",
    "\n",
    "y = np.zeros((len(sections), char_size), dtype = np.float32)\n",
    "\n",
    "#for ecah char in each sentence convert each char to an ID\n",
    "#for each section convert the labels to ids\n",
    "for i,section in enumerate(sections):\n",
    "  for j, char in enumerate(section):\n",
    "    X[i,j, char2id[char]] == 1\n",
    "  y[i, char2id[next_chars[i]]] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine learning\n",
    "\n",
    "batch_size = 512\n",
    "max_steps = 72000\n",
    "log_every = 100\n",
    "save_every = 6000\n",
    "hidden_nodes = 1024\n",
    "\n",
    "test_start = 'I am thinking that'\n",
    "\n",
    "#checkpointing\n",
    "checkpoint_directory = 'checkpoints/wiki'\n",
    "\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "  tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per ephoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-722497e71939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_per_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  global_step = tf.Variable(0)\n",
    "  data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "  labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "  \n",
    "  #input gate, output gate, forget gate, internal state\n",
    "  #They will be calculated in vaccums\n",
    "  \n",
    "  #Input\n",
    "  w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  w_io = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "  #Forget\n",
    "  w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  w_fo = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "  #Output\n",
    "  w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  w_oo = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "  #Memory cell/Hidden\n",
    "  w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  w_co = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "  b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "  def lstm(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i ,w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "    \n",
    "    forget_gate = tf.sigmoid(tf.matmul(i ,w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "    \n",
    "    output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "    \n",
    "    memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "    \n",
    "    state = forget_gate * state + input_get * memory_cell\n",
    "    \n",
    "    output = output_gate * tf.tanh(state)\n",
    "    \n",
    "    return output, state\n",
    "    \n",
    "\n",
    "  #Operation\n",
    "  output = tf.zeros([batch_size, hidden_nodes])\n",
    "  state = tf.zeros([batch_size, hidden_nodes])\n",
    "  \n",
    "  #unrolled lstm loop for each i/p set\n",
    "  for i in range(len_per_section):\n",
    "    output, state = lstm(data[:, i, :], output, state)\n",
    "    if i == 0:\n",
    "      outputs_all_i = output\n",
    "      labels_all_i = data[:, i+1, :]\n",
    "      \n",
    "    elif i != len_per_section -1:\n",
    "      outputs_all_i = tf.concat(0, [outputs_all_i, output])\n",
    "      labels_all_i = tf.concat(0, [labels_all_i, data[:, i+1, :]])\n",
    "      \n",
    "    else:\n",
    "      outputs_all_i = tf.concat(o, [outputs_all_i, output])\n",
    "      labels_all_i = tf.concat(0, [labels_all_i, labels])\n",
    "  \n",
    "  #Classification\n",
    "  w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([char_size]))\n",
    "  \n",
    "  logits = tf.matmul(outputs_all_i, w) + b\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels_all_i))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "#   test_data = tf.placeholder(tf.float32, shape = [1, char_size])\n",
    "#   test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "#   test_state = tf.Variables(tf.zeros([1, hidden_nodes]))\n",
    "  \n",
    "#   reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "#                                 test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "# #   LSTM\n",
    "#   test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "#   test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
